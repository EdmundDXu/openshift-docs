// Module included in the following assemblies:
//
// * nodes/nodes-scheduler-default.adoc

[id="nodes-scheduler-default-about_{context}"]
= Understanding default scheduling

The existing generic scheduler is the default platform-provided scheduler
_engine_ that selects a node to host the pod in a three-step operation:


Filters the Nodes::
The available nodes are filtered based on the constraints or requirements
specified. This is done by running each node through the list of filter
functions called _predicates_.

Prioritize the Filtered List of Nodes::
This is achieved by passing each node through a series of priority_ functions
that assign it a score between 0 - 10, with 0 indicating a bad fit and 10
indicating a good fit to host the pod. The scheduler configuration can also take
in a simple _weight_ (positive numeric value) for each priority function. The
node score provided by each priority function is multiplied by the weight
(default weight for most priorities is 1) and then combined by adding the scores for each node
provided by all the priorities. This weight attribute can be used by
administrators to give higher importance to some priorities.

Select the Best Fit Node::
The nodes are sorted based on their scores and the node with the highest score
is selected to host the pod. If multiple nodes have the same high score, then
one of them is selected at random.

[id="nodes-scheduler-default-about-understanding_{context}"]
== Understanding Scheduler Policy

The selection of the predicate and priorities defines the policy for the scheduler.

The scheduler configuration file is a JSON file that specifies the predicates and priorities the scheduler
will consider.

In the absence of the scheduler policy file, the default scheduler behavior is used.

// we are working on how to configures this in 4.0 right now in https://github.com/openshift/api/pull/181

[IMPORTANT]
====
The predicates and priorities defined in
the scheduler configuration file completely override the default scheduler
policy. If any of the default predicates and priorities are required,
you must explicitly specify the functions in the policy configuration.
====

.Sample scheduler configuration file
[source,json]
----
{
"kind" : "Policy",
"apiVersion" : "v1",
"predicates" : [
	{"name" : "PodFitsHostPorts"},
	{"name" : "PodFitsResources"},
	{"name" : "NoDiskConflict"},
	{"name" : "NoVolumeZoneConflict"},
	{"name" : "MatchNodeSelector"},
	{"name" : "HostName"}
	],
"priorities" : [
	{"name" : "LeastRequestedPriority", "weight" : 1},
	{"name" : "BalancedResourceAllocation", "weight" : 1},
	{"name" : "ServiceSpreadingPriority", "weight" : 1},
	{"name" : "EqualPriority", "weight" : 1}
	]
}
----

[id="nodes-scheduler-default-about-use-cases_{context}"]
== Scheduler Use Cases

One of the important use cases for scheduling within {product-title} is to
support flexible affinity and anti-affinity policies.
ifdef::openshift-enterprise,openshift-origin[]

[id="infrastructure-topological-levels_{context}"]
=== Infrastructure Topological Levels

Administrators can define multiple topological levels for their infrastructure
(nodes) by specifying labels on nodes. For example: `region=r1`, `zone=z1`, `rack=s1`.

These label names have no particular meaning and
administrators are free to name their infrastructure levels anything, such as
city/building/room. Also, administrators can define any number of levels
for their infrastructure topology, with three levels usually being adequate
(such as: `regions` -> `zones` -> `racks`).  Administrators can specify affinity
and anti-affinity rules at each of these levels in any combination.
endif::openshift-enterprise,openshift-origin[]

[id="affinity_{context}"]
=== Affinity

Administrators should be able to configure the scheduler to specify affinity at
any topological level, or even at multiple levels. Affinity at a particular
level indicates that all pods that belong to the same service are scheduled
onto nodes that belong to the same level. This handles any latency requirements
of applications by allowing administrators to ensure that peer pods do not end
up being too geographically separated. If no node is available within the same
affinity group to host the pod, then the pod is not scheduled.

If you need greater control over where the pods are scheduled, see Controlling pod placement on nodes using node affinity rules and
Placing pods relative to other pods using affinity and anti-affinity rules.

These advanced scheduling features allow administrators
to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.


[id="anti-affinity_{context}"]
=== Anti-Affinity

Administrators should be able to configure the scheduler to specify
anti-affinity at any topological level, or even at multiple levels.
Anti-affinity (or 'spread') at a particular level indicates that all pods that
belong to the same service are spread across nodes that belong to that
level. This ensures that the application is well spread for high availability
purposes. The scheduler tries to balance the service pods across all
applicable nodes as evenly as possible.

If you need greater control over where the pods are scheduled, see Controlling pod placement on nodes using node affinity rules and
Placing pods relative to other pods using affinity and anti-affinity rules.

These advanced scheduling features allow administrators
to specify which node a pod can be scheduled on and to force or reject scheduling relative to other pods.
